{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import file_manager as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = fm.get(\"_all_links\")\n",
    "tags = list(all_links.keys())\n",
    "data_block = fm.get(\"_data_block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_text: str) -> list:\n",
    "\t\"\"\"\n",
    "\tTakes raw strings, sterilazes text, uses lemmatization, tokenizes words, and returns tokens in a sorted list.\n",
    "\t\"\"\"\n",
    "\tno_space_text = \" \".join(raw_text.split())  # Remove extra spaces\n",
    "\tno_punctuation_text = re.sub(\"[^0-9A-Za-z ]\", \"\", no_space_text)  # Remove punctuation\n",
    "\tpure_text = \"\".join([i.lower() for i in no_punctuation_text])  #   To lower\n",
    "\n",
    "\t# Tokenize and remove stop words\n",
    "\ttokens = nltk.tokenize.word_tokenize(pure_text)\n",
    "\tstop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\ttokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\t# Lemmatize and sort\n",
    "\tlemm = nltk.stem.WordNetLemmatizer()\n",
    "\ttokens = [lemm.lemmatize(token) for token in tokens if len(token) > 2]\n",
    "\ttokens.sort()\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for key in data_block.keys():\n",
    "\tdata_block[key][\"tokens\"] = tokenize(data_block[key][\"raw\"])\n",
    "\ttokens.append(\" \".join(data_block[key][\"tokens\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['access', 'add', 'algorithm', 'also', 'another', 'api', 'app',\n",
       "       'application', 'array', 'article', 'basic', 'better', 'blog',\n",
       "       'build', 'call', 'called', 'case', 'change', 'check', 'class',\n",
       "       'code', 'coding', 'come', 'command', 'component', 'concept',\n",
       "       'could', 'create', 'created', 'data', 'database', 'day',\n",
       "       'developer', 'development', 'different', 'django', 'dont', 'easy',\n",
       "       'element', 'end', 'environment', 'error', 'even', 'event', 'every',\n",
       "       'example', 'feature', 'file', 'find', 'first', 'following',\n",
       "       'framework', 'function', 'get', 'git', 'github', 'give', 'going',\n",
       "       'good', 'help', 'hope', 'image', 'important', 'input',\n",
       "       'javascript', 'key', 'know', 'language', 'learn', 'learning',\n",
       "       'let', 'library', 'like', 'line', 'list', 'look', 'loop', 'lot',\n",
       "       'make', 'many', 'may', 'mean', 'memory', 'method', 'model', 'much',\n",
       "       'name', 'need', 'new', 'next', 'number', 'object', 'one', 'open',\n",
       "       'operation', 'output', 'page', 'part', 'point', 'post', 'problem',\n",
       "       'process', 'program', 'programming', 'project', 'promise',\n",
       "       'python', 'react', 'request', 'result', 'return', 'run', 'see',\n",
       "       'server', 'set', 'simple', 'software', 'something', 'start',\n",
       "       'statement', 'step', 'store', 'string', 'structure', 'system',\n",
       "       'take', 'task', 'test', 'thats', 'thing', 'time', 'tool', 'try',\n",
       "       'two', 'type', 'understand', 'use', 'used', 'user', 'using',\n",
       "       'value', 'variable', 'want', 'way', 'web', 'well', 'work',\n",
       "       'working', 'would', 'write'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 150)\n",
    "dt_matrix = tfidf.fit_transform(tokens)\n",
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(dt_matrix.toarray(), columns = tfidf.get_feature_names_out())\n",
    "# df.to_csv(\"./data/df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = [\n",
    "\tKMeans(n_clusters = 3),\n",
    "\tKMeans(n_clusters = 4),\n",
    "\tKMeans(n_clusters = 5)\n",
    "]\n",
    "\n",
    "birch = [\n",
    "\tBirch(n_clusters = 4, threshold = 0.3),\n",
    "\tBirch(n_clusters = 4, threshold = 0.5),\n",
    "\tBirch(n_clusters = 4, threshold = 0.7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in kmeans:\n",
    "\tmodel.fit(dt_matrix)\n",
    "\n",
    "for model in birch:\n",
    "\tmodel.fit(dt_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(preds):\n",
    "\tclusters = list(range(-1, max(preds)))\n",
    "\tfor pred in preds:\n",
    "\t\tclusters[pred] = clusters[pred] + 1\n",
    "\treturn sorted(clusters,  reverse = True)\n",
    "\n",
    "def dist_k(kmeans):\n",
    "\tpriorities = [score.index(min(score)) for score in kmeans.cluster_centers_.transpose().tolist()]\n",
    "\treturn distribution(priorities)\n",
    "\n",
    "def dist_b(birch):\n",
    "\tpriorities = [score.index(max(score)) for score in birch.root_.centroids_.transpose().tolist()]\n",
    "\treturn distribution(priorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 40, 37]\n",
      "[55, 42, 30, 25]\n",
      "[54, 37, 32, 21, 11]\n",
      "--------\n",
      "[91, 58]\n",
      "[47, 42, 42, 41, 40, 39, 38, 38, 37, 35, 35, 33, 30, 29, 28, 28, 27, 26, 25, 25, 24, 24, 23, 23, 21, 21, 18, 18, 16, 15, 12, 11, 10, 10, 9, 7, 7, 6, 5, 3, 1, 0]\n",
      "[39, 34, 32, 28, 26, 24, 22, 21, 21, 20, 20, 19, 18, 17, 15, 13, 12, 12, 11, 10, 9, 7, 7, 5, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "for model in kmeans:\n",
    "\tprint(dist_k(model))\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "for model in birch:\n",
    "\tprint(dist_b(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1511, 725, 411]\n",
      "[1044, 827, 418, 360]\n",
      "[1003, 738, 402, 385, 124]\n",
      "--------\n",
      "[1259, 687, 521, 182]\n",
      "[1301, 550, 539, 259]\n",
      "[917, 625, 604, 503]\n"
     ]
    }
   ],
   "source": [
    "kmeans_preds = [model.predict(dt_matrix) for model in kmeans]\n",
    "birch_preds = [model.predict(dt_matrix) for model in birch]\n",
    "\n",
    "for pred in kmeans_preds:\n",
    "\tprint(distribution(pred))\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "for pred in birch_preds:\n",
    "\tprint(distribution(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats()-> None:\n",
    "\tdistribution = sorted([len(all_links[tag]) for tag in all_links.keys()], reverse = True)\n",
    "\tprint(distribution)\n",
    "\tprint(sum(distribution))\n",
    "stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(tokens):\n",
    "\tnum = 0\n",
    "\tfor token in tokens:\n",
    "\t\tif token.count(\"web\") and token.count(\"app\"):\n",
    "\t\t\tnum = num + 1\n",
    "\tprint(num)\n",
    "\t# Or\n",
    "\t# print(len([0 for token in tokens if token.count(\"web\") and token.count(\"app\")]))\n",
    "x(tokens)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2eb9399da6b009ff8750f4b54371a42952ed73383b22791bb9f87bde00d701aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
