{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import file_manager as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = fm.get(\"_all_links\")\n",
    "tags = list(all_links.keys())\n",
    "data_block = fm.get(\"_data_block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_text: str) -> list:\n",
    "\t\"\"\"\n",
    "\tTakes raw strings, sterilazes text, uses lemmatization, tokenizes words, and returns tokens in a sorted list.\n",
    "\t\"\"\"\n",
    "\tno_space_text = \" \".join(raw_text.split())  # Remove extra spaces\n",
    "\tno_punctuation_text = re.sub(\"[^0-9A-Za-z ]\", \"\", no_space_text)  # Remove punctuation\n",
    "\tpure_text = \"\".join([i.lower() for i in no_punctuation_text])  #   To lower\n",
    "\n",
    "\t# Tokenize and remove stop words\n",
    "\ttokens = nltk.tokenize.word_tokenize(pure_text)\n",
    "\tstop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\ttokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\t# Lemmatize and sort\n",
    "\tlemm = nltk.stem.WordNetLemmatizer()\n",
    "\ttokens = [lemm.lemmatize(token) for token in tokens if len(token) > 2]\n",
    "\ttokens.sort()\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for key in data_block.keys():\n",
    "\tdata_block[key][\"tokens\"] = tokenize(data_block[key][\"raw\"])\n",
    "\ttokens.append(\" \".join(data_block[key][\"tokens\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'access', 'add', 'algorithm', 'also', 'always', 'another',\n",
       "       'api', 'app', 'application', 'array', 'article', 'based', 'basic',\n",
       "       'best', 'better', 'block', 'blog', 'browser', 'build', 'call',\n",
       "       'called', 'case', 'change', 'check', 'class', 'code', 'coding',\n",
       "       'come', 'command', 'comment', 'component', 'computer', 'concept',\n",
       "       'copy', 'could', 'create', 'created', 'creating', 'data',\n",
       "       'database', 'day', 'developer', 'development', 'dictionary',\n",
       "       'different', 'django', 'done', 'dont', 'easy', 'element', 'end',\n",
       "       'environment', 'error', 'etc', 'even', 'event', 'every', 'example',\n",
       "       'feature', 'file', 'find', 'first', 'following', 'framework',\n",
       "       'function', 'game', 'get', 'git', 'github', 'give', 'given',\n",
       "       'going', 'good', 'great', 'help', 'hope', 'however', 'html',\n",
       "       'image', 'important', 'information', 'input', 'inside', 'install',\n",
       "       'item', 'javascript', 'keep', 'key', 'know', 'language', 'learn',\n",
       "       'learning', 'let', 'library', 'like', 'line', 'list', 'look',\n",
       "       'loop', 'lot', 'machine', 'main', 'make', 'many', 'may', 'mean',\n",
       "       'memory', 'method', 'might', 'model', 'much', 'multiple', 'must',\n",
       "       'name', 'need', 'new', 'next', 'number', 'object', 'one', 'open',\n",
       "       'operation', 'order', 'output', 'page', 'parameter', 'part',\n",
       "       'people', 'point', 'post', 'problem', 'process', 'program',\n",
       "       'programming', 'project', 'promise', 'property', 'python',\n",
       "       'question', 'react', 'read', 'reading', 'request', 'result',\n",
       "       'return', 'right', 'run', 'search', 'see', 'series', 'server',\n",
       "       'set', 'simple', 'since', 'software', 'solution', 'something',\n",
       "       'source', 'start', 'started', 'state', 'statement', 'step',\n",
       "       'store', 'string', 'structure', 'syntax', 'system', 'take', 'task',\n",
       "       'test', 'thats', 'thing', 'think', 'time', 'tool', 'try',\n",
       "       'tutorial', 'two', 'type', 'understand', 'use', 'used', 'user',\n",
       "       'using', 'value', 'variable', 'version', 'want', 'way', 'web',\n",
       "       'website', 'well', 'without', 'work', 'working', 'world', 'would',\n",
       "       'write'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=200)\n",
    "dt_matrix = tfidf.fit_transform(tokens)\n",
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dt_matrix.toarray(), columns = tfidf.get_feature_names_out())\n",
    "df.to_csv(\"./data/df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans()\n",
    "birch = Birch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2eb9399da6b009ff8750f4b54371a42952ed73383b22791bb9f87bde00d701aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
