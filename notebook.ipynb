{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import file_manager as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = fm.get(\"_all_links\")\n",
    "tags = list(all_links.keys())\n",
    "data_block = fm.get(\"_data_block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_text: str) -> list:\n",
    "\t\"\"\"\n",
    "\tTakes raw strings, sterilazes text, uses lemmatization, tokenizes words, and returns tokens in a sorted list.\n",
    "\t\"\"\"\n",
    "\tno_space_text = \" \".join(raw_text.split())  # Remove extra spaces\n",
    "\tno_punctuation_text = re.sub(\"[^0-9A-Za-z ]\", \"\", no_space_text)  # Remove punctuation\n",
    "\tpure_text = \"\".join([i.lower() for i in no_punctuation_text])  #   To lower\n",
    "\n",
    "\t# Tokenize and remove stop words\n",
    "\ttokens = nltk.tokenize.word_tokenize(pure_text)\n",
    "\tstop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\ttokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\t# Lemmatize and sort\n",
    "\tlemm = nltk.stem.WordNetLemmatizer()\n",
    "\ttokens = [lemm.lemmatize(token) for token in tokens if len(token) > 2]\n",
    "\ttokens.sort()\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for key in data_block.keys():\n",
    "\tdata_block[key][\"tokens\"] = tokenize(data_block[key][\"raw\"])\n",
    "\ttokens.append(\" \".join(data_block[key][\"tokens\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'access', 'action', 'add', 'address', 'algorithm',\n",
       "       'allows', 'already', 'also', 'always', 'another', 'api', 'app',\n",
       "       'application', 'approach', 'argument', 'around', 'array',\n",
       "       'article', 'available', 'back', 'based', 'basic', 'best', 'better',\n",
       "       'block', 'blog', 'branch', 'browser', 'build', 'building', 'call',\n",
       "       'called', 'case', 'change', 'check', 'class', 'click', 'code',\n",
       "       'coding', 'come', 'command', 'comment', 'component', 'computer',\n",
       "       'concept', 'condition', 'consider', 'content', 'control', 'copy',\n",
       "       'could', 'course', 'create', 'created', 'creating', 'cs', 'data',\n",
       "       'database', 'day', 'detail', 'developer', 'development',\n",
       "       'dictionary', 'different', 'django', 'done', 'dont', 'easier',\n",
       "       'easy', 'element', 'end', 'environment', 'error', 'etc', 'even',\n",
       "       'event', 'every', 'everything', 'example', 'execution',\n",
       "       'experience', 'feature', 'feel', 'file', 'find', 'first', 'folder',\n",
       "       'following', 'form', 'framework', 'free', 'function', 'game',\n",
       "       'get', 'getting', 'git', 'github', 'give', 'given', 'going',\n",
       "       'good', 'got', 'great', 'help', 'hope', 'however', 'html', 'idea',\n",
       "       'image', 'import', 'important', 'information', 'input', 'inside',\n",
       "       'install', 'instead', 'issue', 'item', 'ive', 'java', 'javascript',\n",
       "       'job', 'keep', 'key', 'know', 'knowledge', 'language', 'last',\n",
       "       'learn', 'learning', 'let', 'library', 'like', 'line', 'list',\n",
       "       'local', 'logic', 'look', 'loop', 'lot', 'machine', 'made', 'main',\n",
       "       'make', 'making', 'many', 'may', 'mean', 'memory', 'method',\n",
       "       'might', 'model', 'module', 'move', 'much', 'multiple', 'must',\n",
       "       'name', 'need', 'new', 'next', 'node', 'note', 'number', 'object',\n",
       "       'often', 'one', 'open', 'operation', 'operator', 'option', 'order',\n",
       "       'output', 'package', 'page', 'parameter', 'part', 'people',\n",
       "       'performance', 'place', 'point', 'pointer', 'possible', 'post',\n",
       "       'print', 'problem', 'process', 'program', 'programmer',\n",
       "       'programming', 'project', 'promise', 'property', 'provides',\n",
       "       'python', 'question', 'react', 'read', 'reading', 'really',\n",
       "       'reason', 'reference', 'repository', 'request', 'resource',\n",
       "       'result', 'return', 'right', 'run', 'running', 'say', 'science',\n",
       "       'script', 'search', 'second', 'section', 'see', 'series', 'server',\n",
       "       'service', 'set', 'share', 'show', 'similar', 'simple', 'simply',\n",
       "       'since', 'single', 'size', 'skill', 'software', 'solution',\n",
       "       'something', 'source', 'specific', 'stack', 'start', 'started',\n",
       "       'state', 'statement', 'step', 'still', 'store', 'string',\n",
       "       'structure', 'support', 'sure', 'syntax', 'system', 'table',\n",
       "       'take', 'task', 'template', 'test', 'text', 'thats', 'thing',\n",
       "       'think', 'three', 'time', 'today', 'tool', 'topic', 'true', 'try',\n",
       "       'tutorial', 'two', 'type', 'understand', 'understanding', 'update',\n",
       "       'url', 'use', 'used', 'useful', 'user', 'using', 'value',\n",
       "       'variable', 'version', 'video', 'virtual', 'want', 'way', 'web',\n",
       "       'website', 'well', 'window', 'without', 'word', 'work', 'working',\n",
       "       'world', 'would', 'write', 'writing', 'year', 'youll', 'youre'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 300)\n",
    "dt_matrix = tfidf.fit_transform(tokens)\n",
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(dt_matrix.toarray(), columns = tfidf.get_feature_names_out())\n",
    "# df.to_csv(\"./data/df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = [\n",
    "\tKMeans(n_clusters = 3),\n",
    "\tKMeans(n_clusters = 4),\n",
    "\tKMeans(n_clusters = 5)\n",
    "]\n",
    "\n",
    "birch = [\n",
    "\tBirch(n_clusters = 4, threshold = 0.3),\n",
    "\tBirch(n_clusters = 4, threshold = 0.5),\n",
    "\tBirch(n_clusters = 4, threshold = 0.7)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in kmeans:\n",
    "\tmodel.fit(dt_matrix)\n",
    "\n",
    "for model in birch:\n",
    "\tmodel.fit(dt_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(preds):\n",
    "\tclusters = list(range(-1, max(preds)))\n",
    "\tfor pred in preds:\n",
    "\t\tclusters[pred] = clusters[pred] + 1\n",
    "\treturn sorted(clusters, reverse = True)\n",
    "\n",
    "def dist_k(kmeans):\n",
    "\tpriorities = [score.index(min(score)) for score in kmeans.cluster_centers_.transpose().tolist()]\n",
    "\treturn distribution(priorities)\n",
    "\n",
    "def dist_b(birch):\n",
    "\tpriorities = [score.index(min(score)) for score in birch.subcluster_centers_.tolist()]\n",
    "\treturn distribution(priorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144, 96, 60]\n",
      "[143, 64, 48, 47]\n",
      "[152, 50, 37, 36, 30]\n",
      "--------\n",
      "[1711, 180, 40, 10, 5, 3]\n",
      "[1296, 162, 41, 13, 9, 4]\n",
      "[825, 115, 63, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 33, 32, 31, 30, 29, 28, 27, 27, 25, 24, 23, 22, 21, 20, 20, 18, 17, 16, 15, 15, 14, 14, 12, 11, 10, 10, 9, 8, 7, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "for model in kmeans:\n",
    "\tprint(dist_k(model))\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "for model in birch:\n",
    "\tprint(dist_b(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1306, 813, 528]\n",
      "[935, 873, 440, 401]\n",
      "[1084, 610, 471, 341, 146]\n",
      "--------\n",
      "[1427, 744, 417, 61]\n",
      "[1196, 642, 414, 397]\n",
      "[1087, 872, 358, 332]\n"
     ]
    }
   ],
   "source": [
    "kmeans_preds = [model.predict(dt_matrix) for model in kmeans]\n",
    "birch_preds = [model.predict(dt_matrix) for model in birch]\n",
    "\n",
    "for pred in kmeans_preds:\n",
    "\tprint(distribution(pred))\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "for pred in birch_preds:\n",
    "\tprint(distribution(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1182, 792, 554, 147]\n",
      "2675\n"
     ]
    }
   ],
   "source": [
    "def stats()-> None:\n",
    "\tdistribution = sorted([len(all_links[tag]) for tag in all_links.keys()], reverse = True)\n",
    "\tprint(distribution)\n",
    "\tprint(sum(distribution))\n",
    "stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "def x(tokens):\n",
    "\tnum = 0\n",
    "\tfor token in tokens:\n",
    "\t\tif token.count(\"framework\") and token.count(\"python\"):\n",
    "\t\t\tnum = num + 1\n",
    "\tprint(num)\n",
    "x(tokens)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2eb9399da6b009ff8750f4b54371a42952ed73383b22791bb9f87bde00d701aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
